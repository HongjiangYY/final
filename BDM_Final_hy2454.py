# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kF0X3YWR216Ct7vPwfbiBeWECZbLPyac
"""

import csv
import json
import numpy as np
import pandas as pd


import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)
spark
from pyspark.sql.types import DateType, IntegerType, MapType, StringType, FloatType
from pyproj import Transformer
import shapely
from shapely.geometry import Point

# read nyc_cbg_centroids.csv file and make a nyc cbg dictionary, use to find the lon and lat
nyc_cbg_centroids = pd.read_csv('nyc_cbg_centroids.csv')
nyc_cbg_centroids['cbg_fips'] = nyc_cbg_centroids['cbg_fips'].astype(str)
nyc_cbg_centroids.set_index(["cbg_fips"], inplace=True)
nyc_cbg_centroids = nyc_cbg_centroids.transpose()
nyc_cbg_centroids.head()

# read nyc_supermarkets
nyc_supermarkets = pd.read_csv('nyc_supermarkets.csv')
spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()
nyc_supermarkets = spark.createDataFrame(nyc_supermarkets)


weekly_patterns_nyc = spark.read.csv('/tmp/bdm/weekly-patterns-nyc-2019-2020/*', header = True, escape = '"')

## filter out places that not belong to nyc_supermarkets for weekly_patterns_nyc
weekly_patterns_nyc2 = weekly_patterns_nyc.join(nyc_supermarkets, weekly_patterns_nyc['placekey'] == nyc_supermarkets['safegraph_placekey'],'left')\
                                .dropna(subset = ['safegraph_placekey'])\
                                .select('placekey','date_range_start','date_range_end','poi_cbg','visitor_home_cbgs')

## calculate number of visitors and total distance
def cbg_visitor(poi_cbg, visitor_home_cbgs):
  num_visitors = 0
  total_distance = 0
  for k,v in json.loads(visitor_home_cbgs).items():    
    if nyc_cbg_centroids.get(k,None) is not None:
      num_visitors += v
  return int(num_visitors)

def cbg_distance(poi_cbg, visitor_home_cbgs):
  total_distance = 0
  for k,v in json.loads(visitor_home_cbgs).items():    
    if nyc_cbg_centroids.get(k,None) is not None:
      total_distance += Point(nyc_cbg_centroids.get(k, None)[0], nyc_cbg_centroids.get(k, None)[1]).distance(Point(nyc_cbg_centroids.get(poi_cbg, None)[0], nyc_cbg_centroids.get(poi_cbg, None)[1]))/5280
  return float(total_distance)

udfExpand1 = F.udf(cbg_visitor,IntegerType())
udfExpand2 = F.udf(cbg_distance,FloatType())
weekly_patterns_nyc3 = weekly_patterns_nyc2.select('placekey', 'date_range_start', 'date_range_end', 'poi_cbg',
                 udfExpand1(weekly_patterns_nyc2['poi_cbg'], weekly_patterns_nyc2['visitor_home_cbgs']).alias('num_visitor'), \
                 udfExpand2(weekly_patterns_nyc2['poi_cbg'], weekly_patterns_nyc2['visitor_home_cbgs']).alias('num_distance'))

## get 
df_19_03 = weekly_patterns_nyc3.filter(((weekly_patterns_nyc3['date_range_start'] >= '2019-03-01') & (weekly_patterns_nyc3['date_range_start'] <= '2019-03-31')) | ((weekly_patterns_nyc3['date_range_end'] >= '2019-03-01') & (weekly_patterns_nyc3['date_range_end'] <= '2019-03-31')))
df_19_10 = weekly_patterns_nyc3.filter(((weekly_patterns_nyc3['date_range_start'] >= '2019-10-01') & (weekly_patterns_nyc3['date_range_start'] <= '2019-10-31')) | ((weekly_patterns_nyc3['date_range_end'] >= '2019-10-01') & (weekly_patterns_nyc3['date_range_end'] <= '2019-10-31')))
df_20_03 = weekly_patterns_nyc3.filter(((weekly_patterns_nyc3['date_range_start'] >= '2020-03-01') & (weekly_patterns_nyc3['date_range_start'] <= '2020-03-31')) | ((weekly_patterns_nyc3['date_range_end'] >= '2020-03-01') & (weekly_patterns_nyc3['date_range_end'] <= '2020-03-31')))
df_20_10 = weekly_patterns_nyc3.filter(((weekly_patterns_nyc3['date_range_start'] >= '2020-10-01') & (weekly_patterns_nyc3['date_range_start'] <= '2020-10-31')) | ((weekly_patterns_nyc3['date_range_end'] >= '2020-10-01') & (weekly_patterns_nyc3['date_range_end'] <= '2020-10-31')))

df_19_03 = df_19_03.groupBy('poi_cbg').agg(F.collect_list('num_visitor').alias('num_visitor'),F.collect_list('num_distance').alias('num_distance') )
df_20_03 = df_20_03.groupBy('poi_cbg').agg(F.collect_list('num_visitor').alias('num_visitor'),F.collect_list('num_distance').alias('num_distance') )
df_19_10 = df_19_10.groupBy('poi_cbg').agg(F.collect_list('num_visitor').alias('num_visitor'),F.collect_list('num_distance').alias('num_distance') )
df_20_10 = df_20_10.groupBy('poi_cbg').agg(F.collect_list('num_visitor').alias('num_visitor'),F.collect_list('num_distance').alias('num_distance') )

def calAvg(num_visitor, num_distance):
  if sum(num_distance) == 0 or num_visitor == 0:
    return 0
  return sum(num_distance) / sum(num_visitor)


udfExpand = F.udf(calAvg, FloatType())
df_19_03_new = df_19_03.select('poi_cbg', 
                 udfExpand(df_19_03['num_visitor'],df_19_03['num_distance']).alias('2019-03'))
df_19_10_new = df_19_10.select('poi_cbg', 
                 udfExpand(df_19_10['num_visitor'],df_19_10['num_distance']).alias('2019-10'))
df_20_03_new = df_20_03.select('poi_cbg', 
                 udfExpand(df_20_03['num_visitor'],df_20_03['num_distance']).alias('2020-03'))
df_20_10_new = df_20_10.select('poi_cbg', 
                 udfExpand(df_20_10['num_visitor'],df_20_10['num_distance']).alias('2020-10'))

output = df_19_03_new.join(df_19_10_new,'poi_cbg', 'outer')
output = output.join(df_20_03_new, 'poi_cbg', 'outer')
output = output.join(df_20_10_new, 'poi_cbg', 'outer')
output = output.orderBy("poi_cbg")

output.to_csv('input',sep='\t',index=False)

if __name__=='__main__':
  
  sc = SparkContext()
  sc.textFile('input', use_unicode=True) \
        .saveAsTextFile('output')

